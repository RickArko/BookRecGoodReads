{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System - Training & Evaluation\n",
    "\n",
    "This notebook provides:\n",
    "1. Data exploration and visualization\n",
    "2. Model training and parameter tuning\n",
    "3. Evaluation metrics\n",
    "4. Comparison of different approaches\n",
    "5. Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import load_npz\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our modules\n",
    "from hybrid_recommender import HybridRecommender\n",
    "from knn_recommender_sparse import load_sparse_matrix, create_title_mapping, SparseKnnRecommender\n",
    "\n",
    "# Styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's explore our dataset and understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interaction matrix\n",
    "interaction_matrix = load_npz('../data/book_user_matrix_sparse.npz')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Interaction Matrix Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {interaction_matrix.shape} (books x users)\")\n",
    "print(f\"Non-zero entries: {interaction_matrix.nnz:,}\")\n",
    "print(f\"Sparsity: {100 * (1 - interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1])):.2f}%\")\n",
    "print(f\"Density: {100 * interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1]):.4f}%\")\n",
    "print(f\"Average interactions per book: {interaction_matrix.nnz / interaction_matrix.shape[0]:.2f}\")\n",
    "print(f\"Average interactions per user: {interaction_matrix.nnz / interaction_matrix.shape[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of interactions\n",
    "interactions_per_book = np.array(interaction_matrix.sum(axis=1)).flatten()\n",
    "interactions_per_user = np.array(interaction_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Books\n",
    "axes[0].hist(interactions_per_book, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Interactions per Book', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Interactions')\n",
    "axes[0].set_ylabel('Number of Books')\n",
    "axes[0].axvline(interactions_per_book.mean(), color='red', linestyle='--', label=f'Mean: {interactions_per_book.mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Users\n",
    "axes[1].hist(interactions_per_user, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Distribution of Interactions per User', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Interactions')\n",
    "axes[1].set_ylabel('Number of Users')\n",
    "axes[1].axvline(interactions_per_user.mean(), color='red', linestyle='--', label=f'Mean: {interactions_per_user.mean():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Book interaction stats: min={interactions_per_book.min()}, max={interactions_per_book.max()}, median={np.median(interactions_per_book):.0f}\")\n",
    "print(f\"User interaction stats: min={interactions_per_user.min()}, max={interactions_per_user.max()}, median={np.median(interactions_per_user):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content features\n",
    "content_features = load_npz('../data/content_features.npz')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Content Features Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {content_features.shape} (books x features)\")\n",
    "print(f\"Non-zero entries: {content_features.nnz:,}\")\n",
    "print(f\"Sparsity: {100 * (1 - content_features.nnz / (content_features.shape[0] * content_features.shape[1])):.2f}%\")\n",
    "print(f\"Average features per book: {content_features.nnz / content_features.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pl.read_parquet('../data/book_metadata.parquet')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Book Metadata Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total books with metadata: {len(metadata):,}\")\n",
    "print(f\"Average rating: {metadata['average_rating'].mean():.2f}\")\n",
    "print(f\"Average shelves per book: {metadata['num_shelves'].mean():.2f}\")\n",
    "print(f\"Average authors per book: {metadata['num_authors'].mean():.2f}\")\n",
    "print(f\"\\nYear range: {metadata['publication_year'].min()} - {metadata['publication_year'].max()}\")\n",
    "print(f\"Page range: {metadata['num_pages'].min()} - {metadata['num_pages'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training & Parameter Tuning\n",
    "\n",
    "Let's train models with different parameters and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recommenders with different weights\n",
    "import time\n",
    "\n",
    "weight_configs = [\n",
    "    (1.0, 0.0, \"Collaborative Only\"),\n",
    "    (0.0, 1.0, \"Content Only\"),\n",
    "    (0.7, 0.3, \"Collaborative Heavy\"),\n",
    "    (0.6, 0.4, \"Balanced 60/40\"),\n",
    "    (0.5, 0.5, \"Equal Weights\"),\n",
    "    (0.4, 0.6, \"Content Heavy\"),\n",
    "]\n",
    "\n",
    "recommenders = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Training recommenders with different weight configurations...\\n\")\n",
    "\n",
    "for collab_w, content_w, name in weight_configs:\n",
    "    print(f\"Training: {name} (collab={collab_w}, content={content_w})\")\n",
    "    start = time.time()\n",
    "    \n",
    "    recommender = HybridRecommender(\n",
    "        collaborative_weight=collab_w,\n",
    "        content_weight=content_w,\n",
    "        n_neighbors=30\n",
    "    )\n",
    "    \n",
    "    training_times[name] = time.time() - start\n",
    "    recommenders[name] = recommender\n",
    "    \n",
    "    print(f\"  Trained in {training_times[name]:.2f}s\\n\")\n",
    "\n",
    "print(\"✓ All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training times\n",
    "fig = px.bar(\n",
    "    x=list(training_times.keys()),\n",
    "    y=list(training_times.values()),\n",
    "    title='Training Time Comparison',\n",
    "    labels={'x': 'Configuration', 'y': 'Time (seconds)'},\n",
    "    color=list(training_times.values()),\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "\n",
    "Let's evaluate our models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diversity(recommendations, metadata_dict, recommender):\n",
    "    \"\"\"Calculate diversity based on unique genres in recommendations.\"\"\"\n",
    "    unique_genres = set()\n",
    "    total_genres = 0\n",
    "    \n",
    "    for idx, _, _, _ in recommendations:\n",
    "        book_id = recommender.book_ids[idx]\n",
    "        if book_id in metadata_dict:\n",
    "            shelves = metadata_dict[book_id].get('shelves', '')\n",
    "            if shelves:\n",
    "                genres = shelves.split(',')[:5]\n",
    "                unique_genres.update(genres)\n",
    "                total_genres += len(genres)\n",
    "    \n",
    "    if total_genres == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(unique_genres) / total_genres\n",
    "\n",
    "\n",
    "def evaluate_coverage(recommendations, total_books):\n",
    "    \"\"\"Calculate what percentage of catalog appears in recommendations.\"\"\"\n",
    "    unique_books = len(set(idx for idx, _, _, _ in recommendations))\n",
    "    return unique_books / total_books\n",
    "\n",
    "\n",
    "def evaluate_novelty(recommendations, interactions_per_book):\n",
    "    \"\"\"Calculate average novelty (inverse popularity) of recommendations.\"\"\"\n",
    "    novelties = []\n",
    "    max_interactions = interactions_per_book.max()\n",
    "    \n",
    "    for idx, _, _, _ in recommendations:\n",
    "        popularity = interactions_per_book[idx]\n",
    "        novelty = 1 - (popularity / max_interactions)\n",
    "        novelties.append(novelty)\n",
    "    \n",
    "    return np.mean(novelties) if novelties else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all configurations on sample books\n",
    "test_books = [\n",
    "    \"1984\",\n",
    "    \"The Great Gatsby\",\n",
    "    \"Lord of the Rings\",\n",
    "]\n",
    "\n",
    "metadata_dict = {row['book_id']: row for row in metadata.iter_rows(named=True)}\n",
    "\n",
    "results = []\n",
    "\n",
    "for book_query in test_books:\n",
    "    print(f\"\\nEvaluating recommendations for: {book_query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for config_name, recommender in recommenders.items():\n",
    "        # Find book\n",
    "        matches = recommender.fuzzy_search(book_query, threshold=60)\n",
    "        if not matches:\n",
    "            print(f\"  {config_name}: No match found\")\n",
    "            continue\n",
    "        \n",
    "        best_title, book_idx, match_score = matches[0]\n",
    "        \n",
    "        # Get recommendations\n",
    "        recommendations = recommender.recommend_hybrid(book_idx, n_recommendations=20)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_score = np.mean([score for _, score, _, _ in recommendations])\n",
    "        diversity = evaluate_diversity(recommendations, metadata_dict, recommender)\n",
    "        coverage = evaluate_coverage(recommendations, interaction_matrix.shape[0])\n",
    "        novelty = evaluate_novelty(recommendations, interactions_per_book)\n",
    "        \n",
    "        results.append({\n",
    "            'Book': book_query,\n",
    "            'Configuration': config_name,\n",
    "            'Avg Score': avg_score,\n",
    "            'Diversity': diversity,\n",
    "            'Coverage': coverage,\n",
    "            'Novelty': novelty\n",
    "        })\n",
    "        \n",
    "        print(f\"  {config_name:20s} | Score: {avg_score:.3f} | Div: {diversity:.3f} | Nov: {novelty:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Avg Score', 'Diversity', 'Coverage', 'Novelty']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    pivot_data = results_df.pivot(index='Configuration', columns='Book', values=metric)\n",
    "    pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "    \n",
    "    ax.set_title(f'{metric} by Configuration', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Configuration')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend(title='Test Book')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary = results_df.groupby('Configuration')[['Avg Score', 'Diversity', 'Coverage', 'Novelty']].mean()\n",
    "summary = summary.round(3)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Average Metrics Across All Test Books\")\n",
    "print(\"=\" * 80)\n",
    "print(summary.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Find best configuration for each metric\n",
    "print(\"Best Configurations:\")\n",
    "for metric in ['Avg Score', 'Diversity', 'Coverage', 'Novelty']:\n",
    "    best_config = summary[metric].idxmax()\n",
    "    best_value = summary[metric].max()\n",
    "    print(f\"  {metric:15s}: {best_config} ({best_value:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Comparison\n",
    "\n",
    "Let's do a side-by-side comparison of recommendations from different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare recommendations for a specific book\n",
    "query_book = \"1984\"\n",
    "n_show = 5\n",
    "\n",
    "print(f\"\\nComparing Top {n_show} Recommendations for: {query_book}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "comparison_configs = [\n",
    "    (\"Collaborative Only\", recommenders[\"Collaborative Only\"]),\n",
    "    (\"Content Only\", recommenders[\"Content Only\"]),\n",
    "    (\"Balanced 60/40\", recommenders[\"Balanced 60/40\"]),\n",
    "]\n",
    "\n",
    "for config_name, recommender in comparison_configs:\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    matches = recommender.fuzzy_search(query_book, threshold=60)\n",
    "    if not matches:\n",
    "        print(\"  No match found\")\n",
    "        continue\n",
    "    \n",
    "    book_idx = matches[0][1]\n",
    "    recommendations = recommender.recommend_hybrid(book_idx, n_recommendations=n_show)\n",
    "    \n",
    "    for i, (idx, combined, collab, content) in enumerate(recommendations, 1):\n",
    "        title = recommender.idx_to_title[idx]\n",
    "        book_id = recommender.book_ids[idx]\n",
    "        \n",
    "        print(f\"  {i}. {title[:60]}\")\n",
    "        print(f\"     Score: {combined:.3f} (collab: {collab:.3f}, content: {content:.3f})\")\n",
    "        \n",
    "        if book_id in metadata_dict:\n",
    "            meta = metadata_dict[book_id]\n",
    "            genres = meta.get('shelves', '').split(',')[:3]\n",
    "            if genres and genres[0]:\n",
    "                print(f\"     Genres: {', '.join(genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark recommendation speed\n",
    "import time\n",
    "\n",
    "sample_books = list(recommenders[\"Balanced 60/40\"].title_to_idx.keys())[:100]\n",
    "\n",
    "timing_results = {}\n",
    "\n",
    "for config_name, recommender in list(recommenders.items())[:3]:  # Test 3 configs\n",
    "    times = []\n",
    "    \n",
    "    for book_title in sample_books[:20]:  # Test on 20 books\n",
    "        if book_title in recommender.title_to_idx:\n",
    "            book_idx = recommender.title_to_idx[book_title]\n",
    "            \n",
    "            start = time.time()\n",
    "            _ = recommender.recommend_hybrid(book_idx, n_recommendations=10)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            times.append(elapsed * 1000)  # Convert to ms\n",
    "    \n",
    "    timing_results[config_name] = {\n",
    "        'mean': np.mean(times),\n",
    "        'median': np.median(times),\n",
    "        'std': np.std(times)\n",
    "    }\n",
    "\n",
    "print(\"\\nRecommendation Speed (milliseconds):\")\n",
    "print(\"=\" * 60)\n",
    "for config, stats in timing_results.items():\n",
    "    print(f\"{config:25s}: {stats['mean']:.2f}ms (±{stats['std']:.2f}ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations\n",
    "\n",
    "Based on the evaluation, here are the recommendations for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS FOR PRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_overall = summary.mean(axis=1).idxmax()\n",
    "\n",
    "print(f\"\\nBest Overall Configuration: {best_overall}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for metric in ['Avg Score', 'Diversity', 'Coverage', 'Novelty']:\n",
    "    value = summary.loc[best_overall, metric]\n",
    "    print(f\"  {metric:15s}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nUse Cases:\")\n",
    "print(\"  - For accuracy-focused recommendations: Collaborative Heavy (0.7/0.3)\")\n",
    "print(\"  - For diversity and discovery: Content Heavy (0.4/0.6)\")\n",
    "print(\"  - For balanced approach: Balanced 60/40 or Equal Weights\")\n",
    "print(\"  - For cold-start books: Content Only (0.0/1.0)\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best configuration\n",
    "import json\n",
    "\n",
    "# Determine best weights based on metrics\n",
    "best_config = {\n",
    "    'collaborative_weight': 0.6,\n",
    "    'content_weight': 0.4,\n",
    "    'n_neighbors': 30,\n",
    "    'metrics': {\n",
    "        'avg_score': float(summary.loc['Balanced 60/40', 'Avg Score']),\n",
    "        'diversity': float(summary.loc['Balanced 60/40', 'Diversity']),\n",
    "        'coverage': float(summary.loc['Balanced 60/40', 'Coverage']),\n",
    "        'novelty': float(summary.loc['Balanced 60/40', 'Novelty'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/best_model_config.json', 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print(\"✓ Best model configuration saved to: data/best_model_config.json\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(json.dumps(best_config, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
